# -*- coding: utf-8 -*-
"""Recommender System - Anime Movies.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wRvOQ2peYtqByrKHma8OxsHgLAIZ-LsZ

# **0. *Project Info***
- owner: Erwin Gunawan | erwingun03@gmail.com
- Goals: Recommender System
- DataSet: https://www.kaggle.com/datasets/dbdmobile/myanimelist-dataset

# **1. *Library Setup***
"""

!pip install langdetect
!pip install wordcloud
!pip install plotly

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected=True)
from wordcloud import WordCloud
from langdetect import detect
from datetime import datetime

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Dot, Flatten, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping
from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError
from keras import backend as K


from wordcloud import WordCloud
from collections import defaultdict
from collections import Counter

"""# **2. *Data Loading***

in this part data loaded from drive. There are 3 data file that loaded:
- user score data
- user details data
- anime details data
"""

"""
DOWNLOADING Bank Transaction Dataset
"""
! gdown 1c1Y8vkot86g3cQAr4pYJ_8viX5uVpZ7-
! gdown 1KpYNQeoRhRduzIf70lZ0CPextK6zCdsy
! gdown 1MWtl0Ap4s-H7YB-Tt6ZT_8wyGPKOm22h

"""
Load User Score Data
"""
df_score = pd.read_csv('/content/users-score-2023.csv')
df_score.head(5)

"""
Load User Detail Data
"""
df_user = pd.read_csv('/content/users-details-2023.csv')
df_user.head(5)

"""
Load Anime Detail Data
"""
df_anime = pd.read_csv('/content/anime-dataset-2023.csv')
df_anime.head(5)

"""# **3. *Data Understanding***"""

print("User Score Data:")
print(df_score.info())
print("-" * 80)
print("User Detail Data:")
print(df_user.info())
print("-" * 80)
print("Anime Detail Data:")
print(df_anime.info())
print("-" * 80)

"""
Check for duplicates, missing values, and nan values
"""
def check_all_data(df_score, df_user, df_anime):
  duplicates_score = df_score.duplicated().sum()
  duplicates_user = df_user.duplicated().sum()
  duplicates_anime = df_anime.duplicated().sum()

  missing_values_score = df_score.isnull().sum().sum()
  missing_values_user = df_user.isnull().sum().sum()
  missing_values_anime = df_anime.isnull().sum().sum()

  nan_values_score = df_score.isna().sum().sum()
  nan_values_user = df_user.isna().sum().sum()
  nan_values_anime = df_anime.isna().sum().sum()

  if duplicates_score == 0 and duplicates_user == 0 and duplicates_anime == 0 and \
    missing_values_score == 0 and missing_values_user == 0 and missing_values_anime == 0 and \
    nan_values_score == 0 and nan_values_user == 0 and nan_values_anime == 0:
      print("The data is clean.")
  else:
      print("The data has issues:")
      if duplicates_score > 0:
          print(f"There are {duplicates_score} duplicate rows in df_score.")
      if duplicates_user > 0:
          print(f"There are {duplicates_user} duplicate rows in df_user.")
      if duplicates_anime > 0:
          print(f"There are {duplicates_anime} duplicate rows in df_anime.")

      if missing_values_score > 0:
          print(f"There are {missing_values_score} missing values in df_score.")
      if missing_values_user > 0:
          print(f"There are {missing_values_user} missing values in df_user.")
      if missing_values_anime > 0:
          print(f"There are {missing_values_anime} missing values in df_anime.")

      if nan_values_score > 0:
          print(f"There are {nan_values_score} NaN values in df_score.")
      if nan_values_user > 0:
          print(f"There are {nan_values_user} NaN values in df_user.")
      if nan_values_anime > 0:
          print(f"There are {nan_values_anime} NaN values in df_anime.")

check_all_data(df_score, df_user, df_anime)

"""## 3.1 *Exploratory Data Analysis* (EDA)

### 3.1.1 *Univariate Analysis*
"""

"""
User Score Data Rating analysis
"""
df_score['rating'].describe()

import matplotlib.pyplot as plt
import seaborn as sns

# Create a figure with subplots (2 rows, 3 columns)
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Rating Distribution in df_score
sns.histplot(df_score['rating'], bins=10, kde=True, color='teal', palette='mako', ax=axes[0, 0])
axes[0, 0].set_title('Rating Distribution in df_score')
axes[0, 0].set_xlabel('Rating')
axes[0, 0].set_ylabel('Frequency')

# 2. Genre Distribution in df_anime
genres_split = df_anime['Genres'].str.split(', ', expand=True).stack()
genres_count = genres_split.value_counts()
sns.barplot(x=genres_count.index, y=genres_count.values, hue=genres_count.index, ax=axes[0, 1], palette='mako')
axes[0, 1].set_title('Genres Distribution in df_anime')
axes[0, 1].set_xlabel('Genre')
axes[0, 1].set_ylabel('Count')
axes[0, 1].tick_params(axis='x', rotation=90)

# 3. Anime Type Distribution in df_anime
type_count = df_anime['Type'].value_counts()
sns.barplot(x=type_count.index, y=type_count.values, hue=type_count.index, ax=axes[0, 2], palette='mako')
axes[0, 2].set_title('Type Distribution in df_anime')
axes[0, 2].set_xlabel('Type')
axes[0, 2].set_ylabel('Count')

# 4. Producers Distribution in df_anime (Top 20)
producers_count = df_anime['Producers'].value_counts().head(20)
sns.barplot(x=producers_count.index, y=producers_count.values, hue=producers_count.index, ax=axes[1, 0], palette='mako')
axes[1, 0].set_title('Top 20 Producers in df_anime')
axes[1, 0].set_xlabel('Producer')
axes[1, 0].set_ylabel('Count')
axes[1, 0].tick_params(axis='x', rotation=90)

# 5. Gender Distribution in df_user
gender_count = df_user['Gender'].value_counts()
sns.barplot(x=gender_count.index, y=gender_count.values, hue=gender_count.index, ax=axes[1, 1], palette='mako')
axes[1, 1].set_title('Gender Distribution in df_user')
axes[1, 1].set_xlabel('Gender')
axes[1, 1].set_ylabel('Count')

# 6. Location Distribution in df_user (Top 10)
location_count = df_user['Location'].value_counts().head(10)
sns.barplot(x=location_count.index, y=location_count.values, hue=location_count.index, ax=axes[1, 2], palette='mako')
axes[1, 2].set_title('Top 10 Locations in df_user')
axes[1, 2].set_xlabel('Location')
axes[1, 2].set_ylabel('Count')
axes[1, 2].tick_params(axis='x', rotation=90)

plt.tight_layout()
plt.show()

"""### 3.1.2 *Multivariate Analysis*"""

anime_watch_count = df_score.groupby('Anime Title')['user_id'].nunique().reset_index()
anime_watch_count = anime_watch_count.rename(columns={'user_id': 'User Count'})
anime_watch_count = anime_watch_count.sort_values(by='User Count', ascending=False)
top_anime_watch_count = anime_watch_count.head(10)

plt.figure(figsize=(8, 6))
sns.barplot(x='User Count', y='Anime Title', data=top_anime_watch_count, palette='mako')
plt.title('Top 10 Anime Titles Watched by Most Users')
plt.xlabel('Number of Users')
plt.ylabel('Anime Title')
plt.tight_layout()
plt.show()

studio_counts = df_anime['Studios'].value_counts()
studio_counts = studio_counts[studio_counts.index != 'UNKNOWN']
top_studios = studio_counts.head(10)

plt.figure(figsize=(8, 6))
sns.barplot(x=top_studios.index, y=top_studios.values, palette='mako')
plt.title('Number of Animes by Studio (Top 10)')
plt.xlabel('Studio')
plt.ylabel('Number of Animes')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

df_anime_filtered = df_anime.drop(columns=['Image URL', 'Aired', 'Premiered', 'anime_id', 'English name', 'Other name'])

df_anime_filtered = df_anime_filtered.select_dtypes(include=[np.number])

sns.pairplot(df_anime_filtered)
plt.suptitle('Pairplot for df_anime', y=1.02)
plt.show()

categorical_columns = ['Studios', 'Genres', 'Type']

ordinal_encoder = OrdinalEncoder()
df_anime[categorical_columns] = ordinal_encoder.fit_transform(df_anime[categorical_columns])

print(df_anime.head())

correlation_matrix = df_anime[['Popularity', 'Favorites', 'Members', 'Studios', 'Genres', 'Type']].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='mako', fmt='.2f', linewidths=0.5, cbar_kws={'shrink': .8})
plt.title('Correlation Matrix with Ordinal Encoded Features')
plt.show()

"""# **4. *Data Preparation***"""

unique_ratings = df_score['rating'].unique()
print(unique_ratings)
print("-" * 80)
unique_Genres = df_anime['Genres'].unique() #Already convert to numerical feature
print(unique_Genres)
print("-" * 80)
unique_Studios = df_anime['Studios'].unique() #Already convert to numerical feature
print(unique_Studios)
print("-" * 80)

missing_values = df_user.isnull().sum()
features_with_missing = missing_values[missing_values > 0]
print("Features with missing values and NaNs in df_user:")
print(features_with_missing)

missing_values = df_score.isnull().sum()
features_with_missing = missing_values[missing_values > 0]
print("Features with missing values and NaNs in df_score:")
print(features_with_missing)

"""
After knowing alot of thing in EDA.
in this part of data preparation i suggest to:
1. Remove 'Unknown' genres, and Type from df_anime

2. removing missing value / fill it, the data that we have:
There are 232 missing values in df_score.
There are 1648695 missing values in df_user.
There are 232 NaN values in df_score.
There are 1648695 NaN values in df_user.
"""
# Handle UNKNOW VALUE
df_anime_cleaned = df_anime[df_anime['Genres'] != 'Unknown']
df_anime_cleaned = df_anime_cleaned[df_anime_cleaned['Type'] != 'Unknown']

# Handle MISSING VALUE
df_user_cleaned = df_user.drop(columns=['Gender', 'Birthday', 'Location'])
columns_to_impute = ['Days Watched', 'Mean Score', 'Watching', 'Completed',
                     'On Hold', 'Dropped', 'Plan to Watch', 'Total Entries',
                     'Rewatched', 'Episodes Watched']
for col in columns_to_impute:
    df_user_cleaned[col].fillna(df_user_cleaned[col].median(), inplace=True)

df_score_cleaned = df_score.dropna(subset=['Username'])

check_all_data(df_score_cleaned, df_user_cleaned, df_anime_cleaned)

"""a lot better..."""

"""
Scaling our "rating" column by creating  a MinMaxScaler object
Scale the 'score' column between 0 and 1

also Encoding user IDs and anime IDs
"""
scaler = MinMaxScaler(feature_range=(0, 1))

df_score_cleaned['scaled_score'] = scaler.fit_transform(df_score_cleaned[['rating']])

user_encoder = LabelEncoder()
df_score_cleaned["user_encoded"] = user_encoder.fit_transform(df_score_cleaned["user_id"])
num_users = len(user_encoder.classes_)

anime_encoder = LabelEncoder()
df_score_cleaned["anime_encoded"] = anime_encoder.fit_transform(df_score_cleaned["anime_id"])
num_animes = len(anime_encoder.classes_)

print("Number of unique users: {}, Number of unique anime: {}".format(num_users, num_animes))
print("Minimum rating: {}, Maximum rating: {}".format(min(df_score_cleaned['rating']), max(df_score_cleaned['rating'])))

df_score_cleaned = shuffle(df_score_cleaned, random_state=100)

X = df_score_cleaned[['user_encoded', 'anime_encoded']].values
y = df_score_cleaned["scaled_score"].values

print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73)

print("Number of samples in the training set:", len(y_train))
print("Number of samples in the test set:", len(y_test))

X_train_array = [X_train[:, 0], X_train[:, 1]]
X_test_array = [X_test[:, 0], X_test[:, 1]]

"""# **5. *Model Development***"""

"""
construct model for recommender system:
- add User input layer and embedding layer
- add Anime input layer and embedding layer
- add Dense layers for prediction
"""
def root_mean_squared_error(y_true, y_pred):
    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))

def RecommenderNet(num_users, num_animes, embedding_size=128):
    user = Input(name='user_encoded', shape=[1])
    user_embedding = Embedding(
        name='user_embedding', input_dim=num_users, output_dim=embedding_size
    )(user)

    anime = Input(name='anime_encoded', shape=[1])
    anime_embedding = Embedding(
        name='anime_embedding', input_dim=num_animes, output_dim=embedding_size
    )(anime)

    dot_product = Dot(name='dot_product', normalize=True, axes=2)([user_embedding, anime_embedding])
    flattened = Flatten()(dot_product)

    dense = Dense(64, activation='relu')(flattened)
    output = Dense(1, activation='sigmoid')(dense)

    model = Model(inputs=[user, anime], outputs=output)
    model.compile(
        loss='mean_squared_error',
        optimizer=Adam(learning_rate=0.001),
        metrics=[root_mean_squared_error, MeanAbsoluteError()]
    )
    return model

model = RecommenderNet(num_users, num_animes)
model.summary()

"""
for effective and effecient training add checkpoint and early_stopping
"""
checkpoint_filepath = '/content/best_model.weights.h5'
start_lr = 0.00001
min_lr = 0.00001
max_lr = 0.00005
batch_size = 10000

rampup_epochs = 5
sustain_epochs = 0
exp_decay = .8

def lrfn(epoch):
    if epoch < rampup_epochs:
        return (max_lr - start_lr) / rampup_epochs * epoch + start_lr
    elif epoch < rampup_epochs + sustain_epochs:
        return max_lr
    else:
        return (max_lr - min_lr) * exp_decay**(epoch - rampup_epochs - sustain_epochs) + min_lr

lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)

model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,
                                    save_weights_only=True,
                                    monitor='val_loss',
                                    mode='min',
                                    save_best_only=True)

early_stopping = EarlyStopping(patience=3, monitor='val_loss', mode='min', restore_best_weights=True)

my_callbacks = [
    model_checkpoints,
    lr_callback,
    early_stopping
]

history = model.fit(
    x=X_train_array,
    y=y_train,
    batch_size=10000,
    epochs=20,
    verbose=1,
    validation_data=(X_test_array, y_test),
    callbacks=my_callbacks
)

model.load_weights(checkpoint_filepath)

rmse = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

mae = history.history['mean_absolute_error']
val_mae = history.history['val_mean_absolute_error']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(history.epoch))

plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 1)
plt.plot(epochs_range, rmse, label='Training RMSE')
plt.plot(epochs_range, val_rmse, label='Validation RMSE')
plt.legend(loc='upper right')
plt.title('Training and Validation RMSE')

plt.subplot(1, 3, 2)
plt.plot(epochs_range, mae, label='Training MAE')
plt.plot(epochs_range, val_mae, label='Validation MAE')
plt.legend(loc='upper right')
plt.title('Training and Validation MAE')

plt.subplot(1, 3, 3)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.tight_layout()
plt.show()

print("loss: " + str(loss))
print("val_loss: " + str(val_loss))
print("mae: " + str(mae))
print("val_mae: " + str(val_mae))
print("rmse: " + str(rmse))
print("val_rmse: " + str(val_rmse))

"""
Extract weights from the model to use by recoomender function later
"""
def extract_weights(name, model):
    weight_layer = model.get_layer(name)
    weights = weight_layer.get_weights()[0]
    return weights / np.linalg.norm(weights, axis=1, keepdims=True)

anime_weights = extract_weights('anime_embedding', model)
user_weights = extract_weights('user_embedding', model)

"""# **6. *Model Evaluation***

###Evaluation with Item Based Recommendation
"""

"""
Since the "item" refers to an anime movie, we need to set a threshold to recommend only those animes that have been rated by a minimum number of users.

Why? Setting a threshold ensures that the recommended animes have received enough ratings, reflecting a certain degree of popularity or user involvement.
"""

popularity_threshold = 50
df_anime_cleaned= df_anime_cleaned.query('Members >= @popularity_threshold')

"""
Find the top-N similar animes based on cosine similarity of embeddings.

Args:
- name (str): Name of the anime to find similar animes for.
- n (int): Number of similar animes to return.
- return_dist (bool): If True, returns the distance array along with the similar animes.
- neg (bool): If True, returns the least similar animes. Defaults to False.

Returns:
- pd.DataFrame: A DataFrame containing the recommended animes along with similarity scores.
"""
def recommend_animes_by_name(name, n=10, return_dist=False, neg=False):
    try:
        anime_row = df_anime.set_index('Name').loc[name]
        index = anime_row['anime_id']
        encoded_index = anime_encoder.transform([index])[0]

        dists = np.dot(anime_weights, anime_weights[encoded_index])
        sorted_dists = np.argsort(dists)
        closest = sorted_dists[:n] if neg else sorted_dists[-n:]

        print(f'Animes closest to {name}')

        SimilarityArr = []
        for close in closest:
            decoded_id = anime_encoder.inverse_transform([close])[0]
            anime_frame = df_anime[df_anime['anime_id'] == decoded_id]

            anime_name = anime_frame['Name'].values[0]
            english_name = anime_frame['English name'].values[0]
            name_to_use = english_name if english_name != "UNKNOWN" else anime_name
            genre = anime_frame['Genres'].values[0]
            synopsis = anime_frame['Synopsis'].values[0]
            similarity = f"{dists[close] * 100:.2f}%"

            SimilarityArr.append({"Name": name_to_use, "Similarity": similarity, "Genres": genre, "Synopsis": synopsis})

        Frame = pd.DataFrame(SimilarityArr).sort_values(by="Similarity", ascending=False)
        return Frame[Frame.Name != name]

    except KeyError:
        print(f'{name} not found in Anime list')

pd.set_option('display.max_colwidth', None)

recommend_animes_by_name('Mushoku Tensei: Isekai Ittara Honki Dasu', n=5, neg=False)

"""###Evaluation with User Based Recommendation"""

"""
This function identifies users who are most similar to a given input user based on their anime preferences.
It computes the similarity using a weighted matrix and returns a DataFrame with the most similar users.
The function allows you to specify the number of similar users to find (`n`), and optionally return the similarity
distance or find the least similar users.

Parameters:
- item_input (int): User ID of the input user.
- n (int): Number of similar users to return (default is 10).
- return_dist (bool): If True, returns the similarity distance along with the results (default is False).
- neg (bool): If True, returns the least similar users (default is False).

Returns:
- pd.DataFrame: A DataFrame with similar users and their similarity scores, sorted by similarity.
"""
def find_similar_users(item_input, n=10, return_dist=False, neg=False):
    try:
        if item_input not in user_encoder.classes_:
            raise ValueError(f"{item_input} not found in User list")

        encoded_index = user_encoder.transform([item_input])[0]
        dists = np.dot(user_weights, user_weights[encoded_index])
        sorted_dists = np.argsort(dists)

        closest = sorted_dists[:n] if neg else sorted_dists[-n:]

        SimilarityArr = [{"similar_users": user_encoder.inverse_transform([close])[0], "similarity": dists[close]} for close in closest]

        Frame = pd.DataFrame(SimilarityArr).sort_values(by="similarity", ascending=False)

        return Frame

    except ValueError as ve:
        print(f'\033[1m{ve}\033[0m')
        return pd.DataFrame()  # Return empty DataFrame if the user is not found

ratings_per_user = df_score_cleaned.groupby('user_id').size()
random_user = int(ratings_per_user[ratings_per_user < 500].sample(1, random_state=None).index[0])

similar_users = find_similar_users(random_user, n=2, neg=False)
similar_users = similar_users[similar_users.similarity > 0.4]
similar_users = similar_users[similar_users.similar_users != random_user]
similar_users

def showWordCloud(all_genres):
    genres_cloud = WordCloud(width=700, height=400, background_color='white', colormap='gnuplot').generate_from_frequencies(all_genres)
    plt.figure(figsize=(10, 8))
    plt.imshow(genres_cloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

"""
This function retrieves a user's anime preferences by analyzing their top-rated animes.
It returns a DataFrame of these animes along with their genres, and optionally provides a word cloud visualization
of the genres they prefer.

Parameters:
- user_id (int): The ID of the user whose preferences are to be retrieved.
- plot (bool): If True, displays a word cloud visualization of the user's preferred genres (default is False).
- verbose (int): Controls the verbosity of the output. If non-zero, additional user details are printed (default is 0).

Returns:
- pd.DataFrame: A DataFrame containing the titles and genres of the top-rated animes watched by the user.
"""
def get_user_preferences(user_id, plot=False, verbose=0):
    # Filter animes watched by the user
    animes_watched_by_user = df_score_cleaned[df_score_cleaned['user_id'] == user_id]

    if animes_watched_by_user.empty:
        print(f"User #{user_id} has not watched any animes.")
        return pd.DataFrame()

    # Calculate the user's top-rated animes based on the 75th percentile rating
    user_rating_percentile = np.percentile(animes_watched_by_user['rating'], 75)
    animes_watched_by_user = animes_watched_by_user[animes_watched_by_user['rating'] >= user_rating_percentile]

    # Sort and extract the top-rated anime IDs
    top_animes_user = animes_watched_by_user.sort_values(by="rating", ascending=False)['anime_id'].values

    # Retrieve anime details for the top-rated animes
    anime_df_rows = df_anime[df_anime['anime_id'].isin(top_animes_user)][['Name', 'Genres']]

    if verbose:
        avg_rating = animes_watched_by_user['rating'].mean()
        print(f"User \033[1m{user_id}\033[0m has watched {len(animes_watched_by_user)} anime(s) with an average rating of {avg_rating:.1f}/10\n")
        print('\033[1m----- Preferred genres----- \033[0m\n')

    # Plot the word cloud of genres if requested
    if plot:
        genres_list = [genre.strip() for genres in anime_df_rows['Genres'] if isinstance(genres, str) for genre in genres.split(',')]
        showWordCloud(dict(Counter(genres_list)))

    return anime_df_rows

user_pref = get_user_preferences(random_user, plot=True, verbose=1)
pd.DataFrame(user_pref).head(5)

"""
This function recommends anime titles to a selected user based on the preferences of users who are similar.
It filters out animes that the selected user has already watched and identifies new recommendations based on the preferences of similar users.

Parameters:
- similar_users (pd.DataFrame): DataFrame containing similar users to the selected user.
- user_pref (pd.DataFrame): DataFrame of the selected user's anime preferences.
- n (int): Number of anime recommendations to return (default is 5).

Returns:
- pd.DataFrame: A DataFrame containing the recommended animes along with their genres, synopsis, and the number of users who have watched them.
"""
def recommend_animes_by_user(similar_users, user_pref, n=5):
    recommended_animes = []
    anime_list = []

    for user_id in similar_users.similar_users.values:
        pref_list = get_user_preferences(int(user_id))
        if not pref_list.empty:  # Check if user has watched any animes
            pref_list = pref_list[~pref_list["Name"].isin(user_pref["Name"].values)]
            anime_list.append(pref_list.Name.values)

    if len(anime_list) == 0:
        print("No anime recommendations available for the given users.")
        return pd.DataFrame()

    anime_list = pd.DataFrame(anime_list)
    sorted_list = pd.DataFrame(pd.Series(anime_list.values.ravel()).value_counts()).head(n)
    anime_count = df_score_cleaned['anime_id'].value_counts()

    for i, anime_name in enumerate(sorted_list.index):
        if isinstance(anime_name, str):
            try:
                anime_id = df_anime[df_anime.Name == anime_name].anime_id.values[0]
                english_name = df_anime[df_anime['Name'] == anime_name]['English name'].values[0]
                name = english_name if english_name != "UNKNOWN" else anime_name
                genre = df_anime[df_anime.Name == anime_name].Genres.values[0]
                Synopsis = df_anime[df_anime.Name == anime_name].Synopsis.values[0]
                n_user_pref = anime_count.get(anime_id, 0)
                recommended_animes.append({
                    "n": n_user_pref,
                    "anime_name": anime_name,
                    "Genres": genre,
                    "Synopsis": Synopsis
                })
            except:
                pass
    return pd.DataFrame(recommended_animes)

recommended_animes = recommend_animes_by_user(similar_users, user_pref, n=10)

print('\n> Top recommendations for user: {}'.format(random_user))
recommended_animes